# Axiom environment template (subset)

# Phase 35: Tripwire extensions
AXIOM_TRIPWIRE_MAX_HITS=200
AXIOM_TRIPWIRE_LATENCY_MS=5000
AXIOM_TRIPWIRE_MAX_ARBITRATION_RETRIES=3
AXIOM_TRIPWIRE_MAX_PROVENANCE_FAILURES=10
AXIOM_TRIPWIRE_OVERRIDE=0

# Legacy Tripwires (existing)
AXIOM_TRIPWIRE_CONTRADICTIONS=50
AXIOM_TRIPWIRE_CHURN=500

##############################
# Axiom Environment Template #
##############################

# Phase 33: Provenance-Aware Retrieval and Ranking
# Require provenance for writes and retrievals (fail-closed by default)
AXIOM_PROVENANCE_REQUIRED=1
# Penalty applied to ranking when provenance is missing and required
AXIOM_PROVENANCE_PENALTY=0.2

########################################
# Axiom Environment Template
# This file documents recommended defaults. Copy to `.env` and adjust.
########################################

# --- Security ---
# AXIOM_AUTH_ENABLED=false
# AXIOM_AUTH_TOKEN=

# --- Vector/Qdrant ---
# IMPORTANT: In the standard deployment, the **Vector pod Qdrant HTTP API** is exposed on port **8011**
# (it may coexist with an empty dev Qdrant on :6333). Prefer configuring the Vector pod endpoint via:
# VECTOR_POD_URL=http://<vector-pod-host>:8011
#
# Use QDRANT_URL only when you explicitly want to point directly at a specific Qdrant instance
# (cloud, or a known-good self-host). If you set QDRANT_URL, it takes precedence over VECTOR_POD_URL.
# QDRANT_URL=
# VECTOR_POD_URL=http://localhost:8011
#
# Optional: remote embedding service (so the LLM pod can do vector recall without torch/sentence-transformers)
# AXIOM_EMBEDDING_URL=http://<vector-pod-host>:8020
# Canonical similarity threshold for recall filtering (default 0.30; useful for Qdrant cosine scores ~0.30–0.34)
# AXIOM_RETRIEVAL_MIN_SIM=0.30
#
# Local embeddings are disabled by default. Only enable SentenceTransformer when you *really* want local embeddings
# (and expect model downloads) and you do NOT have AXIOM_EMBEDDING_URL set.
# AXIOM_USE_SENTENCE_TRANSFORMERS=false
# QDRANT_MEMORY_COLLECTION=axiom_memories
# QDRANT_BELIEF_COLLECTION=axiom_beliefs

# --- Provenance (Phase 32) ---
# Fail-closed by default: require provenance on memory/belief upserts
AXIOM_PROVENANCE_REQUIRED=1

# --- Adapter ---
# VECTOR_PATH=adapter
# VECTOR_ADAPTER_URL=http://localhost:5001

# --- Feature flags (selected) ---
# ADAPTER_ENABLE_V1_WRITES=false
# AXIOM_JUDGER_ENABLED=1
# AXIOM_CONFIDENCE_ENABLED=1
# AXIOM_CONTRADICTION_ENABLED=0

###############################################
# Axiom Environment Template
#
# Copy to .env and adjust values locally.
# This file includes Phase 30 Arbitration flags (OFF by default).
###############################################

# Phase 30 – Arbitration & Prioritization (OFF by default)
AXIOM_ARBITRATION_ENABLED=0
AXIOM_ARBITRATION_MODE=uniform   # uniform | context | recency | procedural_bias
AXIOM_ARBITRATION_INTENT_MODEL=heuristic  # heuristic | llm

# Base weights for context mode (sum need not be 1; normalized at runtime)
AXIOM_ARB_W_BASE=0.30
AXIOM_ARB_W_EPISODIC=0.25
AXIOM_ARB_W_PROCEDURAL=0.30
AXIOM_ARB_W_ABSTRACTION=0.30

# Intent-specific multipliers (applied on top of base; normalized)
# HOW → prioritize procedural, then episodic
AXIOM_ARB_MULT_HOW_BASE=0.8
AXIOM_ARB_MULT_HOW_EPISODIC=1.0
AXIOM_ARB_MULT_HOW_PROCEDURAL=1.5
AXIOM_ARB_MULT_HOW_ABSTRACTION=0.9

# WHY → prioritize abstraction/causal, then episodic
AXIOM_ARB_MULT_WHY_BASE=0.8
AXIOM_ARB_MULT_WHY_EPISODIC=1.0
AXIOM_ARB_MULT_WHY_PROCEDURAL=0.9
AXIOM_ARB_MULT_WHY_ABSTRACTION=1.5

# WHAT/WHEN/WHO (factual/contextual) → base + episodic bias
AXIOM_ARB_MULT_FACT_BASE=1.3
AXIOM_ARB_MULT_FACT_EPISODIC=1.3
AXIOM_ARB_MULT_FACT_PROCEDURAL=0.9
AXIOM_ARB_MULT_FACT_ABSTRACTION=0.9

# Conflict handling
AXIOM_ARB_CONFLICT_RESOLUTION=hierarchical  # hierarchical | confidence | recency | uncertain
AXIOM_ARB_UNCERTAIN_THRESHOLD=0.15          # within this margin → mark "uncertain"

#############################
# Axiom Environment Template #
#############################

# Cross-consolidation reasoning
AXIOM_CROSS_CONSOLIDATION_ENABLED=0
AXIOM_CROSS_CONSOLIDATION_BOOST=0.1

# Procedural abstraction
AXIOM_PROCEDURAL_LOOP_ENABLED=0      # 0=off, 1=on
AXIOM_PROCEDURAL_WINDOW_TURNS=100    # how many recent turns to scan
AXIOM_PROCEDURAL_MIN_SUPPORT=3       # min supporting sequences to form a procedure

# Core env template for Axiom/SteveBot

# Episodic consolidation
AXIOM_EPISODIC_LOOP_ENABLED=0        # 0=off, 1=on
AXIOM_EPISODIC_WINDOW_HOURS=24       # sliding window size (hours)
AXIOM_EPISODIC_MIN_LINKS=3           # minimum related memories to form an episode
AXIOM_EPISODIC_TICK_SEC=3600         # scheduler tick (seconds)
AXIOM_EPISODIC_MIN_INTERVAL_SEC=10800 # minimum seconds between successful runs

# Memory Hygiene (OFF by default)
AXIOM_HYGIENE_ENABLED=0
AXIOM_HYGIENE_TICK_SEC=7200
AXIOM_HYGIENE_MIN_INTERVAL_SEC=86400
AXIOM_HYGIENE_DRY_RUN=1
AXIOM_HYGIENE_ARCHIVE_THRESHOLD=0.3
AXIOM_HYGIENE_RETIRE_THRESHOLD=0.1

# Optional tuning knobs
AXIOM_HYGIENE_STALE_SEC=7776000
AXIOM_HYGIENE_CONTRA_WINDOW_SEC=1209600
AXIOM_HYGIENE_TRIPWIRE_PCT=0.7
AXIOM_HYGIENE_RECENCY_HALFLIFE_SEC=2592000
# Meta-Cognition – OFF by default
AXIOM_META_LOOP_ENABLED=0
AXIOM_META_TICK_SEC=1800
AXIOM_META_MIN_INTERVAL_SEC=3600
AXIOM_META_WINDOW=24h
AXIOM_META_DRY_RUN=1
AXIOM_META_WEIGHTS="retrieval=0.35,contradiction=0.25,abstraction=0.20,dream=0.20"

# CHAMP meta-blend – OFF by default
AXIOM_CHAMP_META_ENABLED=0
AXIOM_CHAMP_META_WEIGHT=0.15

# Axiom Environment Template
# Copy to .env and adjust as needed.

# Phase 21: Dream Loop (offline consolidation) – OFF and DRY-RUN by default
AXIOM_DREAM_LOOP_ENABLED=0
AXIOM_DREAM_TICK_SEC=900
AXIOM_DREAM_MIN_INTERVAL_SEC=1800
AXIOM_DREAM_WINDOW=7d
AXIOM_DREAM_MAX_ITEMS=200
AXIOM_DREAM_MAX_CONSOLIDATIONS=10
AXIOM_DREAM_MAX_TOKENS=2000
AXIOM_DREAM_MAX_MS=4000
AXIOM_DREAM_DRY_RUN=1

# Axiom RECALL configuration template
# Copy to .env and adjust as needed. Defaults are safe and match code behavior.

# ──────────────────────────────────────────────────────────────────────────────
# General
# ──────────────────────────────────────────────────────────────────────────────
# Enable conservative query simplification (planner fallback)
AXIOM_QUERY_SIMPLIFY_ENABLED=1
# Enable subject-first extraction in planner
AXIOM_SUBJECT_EXTRACTION_ENABLED=1
# Top-1 fallback when retrieval is empty
AXIOM_TOP1_FALLBACK=1
# Minimum score to allow top-1 fallback candidate
AXIOM_TOP1_MIN_SCORE=0.05
# Global similarity threshold used by vector selection (0..1)
SIMILARITY_THRESHOLD=0.3
# Emit structured RECALL diagnostics (JSON) for debugging
RECALL_DIAGNOSTICS=false


# ──────────────────────────────────────────────────────────────────────────────
# Ranking (Phase 6 logic, but used broadly)
# ──────────────────────────────────────────────────────────────────────────────
# Blend weights for final ranking: vector, confidence, judger
AXIOM_RETRIEVAL_WEIGHTS="vector=0.4,confidence=0.3,judger=0.3"
# Drop hits below this final blended score
AXIOM_RETRIEVAL_MIN_SCORE=0.25
# Use belief confidence only for final score (ignores vector/judger)
AXIOM_CONFIDENCE_ONLY_RANKING=0


# ──────────────────────────────────────────────────────────────────────────────
# Judger (Phase 3/4/10)
# ──────────────────────────────────────────────────────────────────────────────
# Enable LLM Memory Judger
AXIOM_JUDGER_ENABLED=1
# Drop items below this score (0..1)
AXIOM_JUDGER_THRESHOLD=0.3
# Max items passed to judger prompt
AXIOM_JUDGER_MAX_ITEMS=12
# Logical model label for reranker (connector uses primary LLM by default)
AXIOM_JUDGER_MODEL=primary


# ──────────────────────────────────────────────────────────────────────────────
# Graph (Phase 2/4/7/10)
# ──────────────────────────────────────────────────────────────────────────────
# Enable belief graph augmentation/upserts
AXIOM_BELIEF_GRAPH_ENABLED=0
# Backend selector for belief graph
AXIOM_BELIEF_BACKEND=sqlite
# SQLite DB path for belief graph
AXIOM_BELIEF_SQLITE_PATH=beliefs/belief_graph.sqlite
# Traversal depth for associative retrieval
AXIOM_BELIEF_GRAPH_TRAVERSAL_DEPTH=2
# Modern alias for traversal depth (associative)
AXIOM_ASSOCIATIVE_DEPTH=2


# ──────────────────────────────────────────────────────────────────────────────
# Semantic (Phase 10)
# ──────────────────────────────────────────────────────────────────────────────
# Enable semantic expansion fanout
AXIOM_SEMANTIC_EXPANSION_ENABLED=1
# Expansion mode: llm | embedding
AXIOM_SEMANTIC_EXPANSION_MODE=llm
# Expansion timing: before | after | disabled
AXIOM_SEMANTIC_EXPANSION_STRATEGY=before


# ──────────────────────────────────────────────────────────────────────────────
# Temporal (Phase 11/12/15)
# ──────────────────────────────────────────────────────────────────────────────
# Enable temporal operators and sequencing
AXIOM_TEMPORAL_SEQUENCING_ENABLED=1
# Prefer recent items when scores tie
AXIOM_TEMPORAL_BIAS_RECENT=1
# Enable natural language time parsing ("yesterday", "last week")
AXIOM_NL_TIME_PARSING_ENABLED=1
# Allow use of python-dateparser if installed
AXIOM_DATEPARSER_ENABLED=1


# ──────────────────────────────────────────────────────────────────────────────
# Causal (Phase 13)
# ──────────────────────────────────────────────────────────────────────────────
# Enable causal retrieval operators (why:, because_of:)
AXIOM_CAUSAL_REASONING_ENABLED=0
# Causal edge extraction mode: llm | rules | disabled
AXIOM_CAUSAL_EXTRACTION_MODE=llm


# ──────────────────────────────────────────────────────────────────────────────
# Counterfactual (Phase 14)
# ──────────────────────────────────────────────────────────────────────────────
# Enable counterfactual reasoning operators (what_if:, if_not:)
AXIOM_COUNTERFACTUAL_ENABLED=1
# Minimum edge confidence used in counterfactual simulation
AXIOM_COUNTERFACTUAL_MIN_CONFIDENCE=0.4


# ──────────────────────────────────────────────────────────────────────────────
# Reflection (Phase 8/9)
# ──────────────────────────────────────────────────────────────────────────────
# Enable reflection and journaling
AXIOM_REFLECTION_ENABLED=1
# Enable autonomous reflection cycles
AXIOM_REFLECTION_CYCLES_ENABLED=0
# Conversation turns between auto cycles
AXIOM_REFLECTION_CYCLE_INTERVAL=50
# Number of recent journals to review during reflection
AXIOM_REFLECTION_REVIEW_WINDOW=10


# ──────────────────────────────────────────────────────────────────────────────
# Confidence & Decay (Phase 5)
# ──────────────────────────────────────────────────────────────────────────────
# Enable belief confidence/decay dynamics
AXIOM_CONFIDENCE_ENABLED=1
# Inactive below this threshold
AXIOM_CONFIDENCE_MIN_THRESHOLD=0.2
# Recency half-life (days) used by some components
AXIOM_DECAY_HALFLIFE_DAYS=90
# Alternate name used by memory decay policy (days)
# Note: Preferred knob is AXIOM_DECAY_HALFLIFE_DAYS; this remains for compatibility
AXIOM_DECAY_HALF_LIFE=30


# ──────────────────────────────────────────────────────────────────────────────
# Contradictions (Phase 4)
# ──────────────────────────────────────────────────────────────────────────────
# Enable lightweight contradiction scan on upsert/retrieval
AXIOM_CONTRADICTION_ENABLED=0
# Deprecated: Some older branches use AXIOM_CONTRADICTIONS (boolean)
# AXIOM_CONTRADICTIONS is deprecated; use AXIOM_CONTRADICTION_ENABLED instead.
# AXIOM_CONTRADICTIONS=0

# RECALL Phase 14: Counterfactual Reasoning
# Enable counterfactual reasoning and set minimum edge confidence for simulation
AXIOM_COUNTERFACTUAL_ENABLED=1
AXIOM_COUNTERFACTUAL_MIN_CONFIDENCE=0.4

# RECALL Phase 13: Causal Linking & Reasoning
# Enable causal operator traversal and extraction
AXIOM_CAUSAL_REASONING_ENABLED=0
# llm | rules | disabled
AXIOM_CAUSAL_EXTRACTION_MODE=llm

#
# Axiom / Stevebot Environment Template
#
# Copy this file to .env and adjust values for your environment.
# This template only includes a minimal subset used in tests and docs.

# Vector / Memory endpoints (examples)
MEMORY_POD_URL=http://localhost:8002
# Prefer the Vector pod Qdrant API (real data is typically exposed on :8011 in prod)
VECTOR_POD_URL=http://localhost:8011
# QDRANT_URL=  # optional explicit override (cloud / direct Qdrant)

# RECALL Phases
# Phase 11: Temporal Sequencing
AXIOM_TEMPORAL_SEQUENCING_ENABLED=1
AXIOM_TEMPORAL_BIAS_RECENT=1

# Phase 12: Natural Language → Temporal Operators
AXIOM_NL_TIME_PARSING_ENABLED=1
AXIOM_DATEPARSER_ENABLED=1

#
# Axiom (.env.template)
# Copy to .env and edit values locally. Do NOT commit your .env.
#

# ---- RECALL Phase 11: Temporal Sequencing ----
# Enable operator-based temporal sequencing in retrieval queries.
# Supported operators in query text:
#   before:YYYY-MM-DD   after:YYYY-MM-DD   since:Nd   sort:asc|desc
AXIOM_TEMPORAL_SEQUENCING_ENABLED=1

# When final scores tie, bias the order toward more recent items.
AXIOM_TEMPORAL_BIAS_RECENT=1

# ---- RECALL Phase 10 (Semantic) ----
# AXIOM_SEMANTIC_EXPANSION_ENABLED=1
# AXIOM_SEMANTIC_EXPANSION_MODE=llm        # llm | embedding
# AXIOM_SEMANTIC_EXPANSION_STRATEGY=before # before | after | disabled
# AXIOM_ASSOCIATIVE_DEPTH=2

# ---- Belief Graph ----
# AXIOM_BELIEF_GRAPH_ENABLED=1
# AXIOM_BELIEF_BACKEND=sqlite  # sqlite | neo4j
# AXIOM_BELIEF_SQLITE_PATH=beliefs/belief_graph.sqlite

# ---- Ranking & Confidence ----
# AXIOM_RETRIEVAL_MIN_SCORE=0.25
# AXIOM_CONFIDENCE_ENABLED=1
# AXIOM_CONFIDENCE_MIN_THRESHOLD=0.2
# AXIOM_DECAY_HALFLIFE_DAYS=90

# Axiom / Stevebot environment template (subset)

# ── RECALL Phase 10: Semantic & Associative Retrieval ─────────────────────────
# Enable semantic expansion of queries (fan-out before/after vector search)
AXIOM_SEMANTIC_EXPANSION_ENABLED=1
# Expansion mode: llm (strict JSON) or embedding (tag/subject fallback)
AXIOM_SEMANTIC_EXPANSION_MODE=llm
# Strategy: before | after | disabled
AXIOM_SEMANTIC_EXPANSION_STRATEGY=before

# Associative retrieval depth for belief graph traversal
AXIOM_ASSOCIATIVE_DEPTH=2

# RECALL Phases - Core
AXIOM_REFLECTION_ENABLED=1
AXIOM_REFLECTION_INTERVAL=10
AXIOM_CONFIDENCE_ENABLED=1

# RECALL Phase 9 – Autonomous Reflection Cycles
AXIOM_REFLECTION_CYCLES_ENABLED=0
AXIOM_REFLECTION_CYCLE_INTERVAL=50
AXIOM_REFLECTION_REVIEW_WINDOW=10
# Axiom / Stevebot Environment Template
# Copy to .env and customize for your environment

# ─────────────────────────────────────────────────────────────
# LLM Provider (see README for details)
# LLM_PROVIDER=ollama|openai|openai_compatible
# OLLAMA_URL=http://127.0.0.1:11434
# LLM_API_BASE=https://api.openai.com/v1
# OPENAI_API_KEY=
# LLM_MODEL_ID=

# ─────────────────────────────────────────────────────────────
# Vector / Qdrant
# QDRANT_URL=http://localhost:6333

# ─────────────────────────────────────────────────────────────
# RECALL configuration
AXIOM_REFLECTION_ENABLED=1
AXIOM_REFLECTION_INTERVAL=10
# Belief confidence/decay (enables reinforcement updates during reflection)
AXIOM_CONFIDENCE_ENABLED=1
# Journals directory (optional override)
# AXIOM_JOURNALS_DIR=/workspace/journals

# Judger (Phase 3)
AXIOM_JUDGER_ENABLED=1
AXIOM_JUDGER_THRESHOLD=0.3
AXIOM_JUDGER_MODEL=primary

# Contradictions (Phase 4 / 7)
# AXIOM_CONTRADICTION_ENABLED=1
# AXIOM_CONTRADICTION_RESOLUTION=0
# AXIOM_RESOLUTION_STRATEGY=confidence

# Confidence & Decay (Phase 5)
# AXIOM_CONFIDENCE_MIN_THRESHOLD=0.2
# AXIOM_DECAY_HALFLIFE_DAYS=90
# AXIOM_CONFIDENCE_WEIGHTS="recency=0.3, reinforcement=0.3, source=0.2, contradiction=0.2"

# Context budgets / allocator (optional)
# CONTEXT_ALLOCATOR_ENABLED=1
# CONTEXT_TOKEN_BUDGET=6000

# Logging
# LOG_LEVEL=INFO